import gradio as gr
import os
from pydub import AudioSegment
from pydub.effects import normalize, compress_dynamic_range
import speech_recognition as sr
from pathlib import Path
import shutil
import json
import librosa
import numpy as np
import soundfile as sf
from collections import Counter
import matplotlib.pyplot as plt
import io
import base64
from datetime import datetime

def process_audio(audio_file, language, chunk_duration, normalize_audio, remove_silence, 
                  apply_noise_reduction, min_duration, max_duration, progress=gr.Progress()):
    """Process audio file: split into chunks and transcribe each"""
    if audio_file is None:
        return "‡§ï‡•É‡§™‡§Ø‡§æ ‡§™‡§π‡§≤‡•á ‡§è‡§ï ‡§ë‡§°‡§ø‡§Ø‡•ã ‡§´‡§º‡§æ‡§á‡§≤ ‡§Ö‡§™‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç! / Please upload an audio file first!", None, None, None
    
    try:
        output_dir = "tts_training_data"
        audio_chunks_dir = os.path.join(output_dir, "audio_chunks")
        filtered_chunks_dir = os.path.join(output_dir, "filtered_chunks")
        stats_dir = os.path.join(output_dir, "statistics")
        os.makedirs(audio_chunks_dir, exist_ok=True)
        os.makedirs(filtered_chunks_dir, exist_ok=True)
        os.makedirs(stats_dir, exist_ok=True)
        
        progress(0, desc="‡§ë‡§°‡§ø‡§Ø‡•ã ‡§´‡§º‡§æ‡§á‡§≤ ‡§≤‡•ã‡§° ‡§π‡•ã ‡§∞‡§π‡•Ä ‡§π‡•à... / Loading audio file...")
        
        audio = AudioSegment.from_file(audio_file)
        audio = audio.set_channels(1)
        audio = audio.set_frame_rate(22050)
        
        if normalize_audio:
            progress(0.05, desc="‡§ë‡§°‡§ø‡§Ø‡•ã ‡§®‡•â‡§∞‡•ç‡§Æ‡§≤‡§æ‡§á‡§ú‡§º ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•à... / Normalizing audio...")
            audio = normalize(audio)
            audio = compress_dynamic_range(audio)
        
        duration_ms = len(audio)
        chunk_length_ms = int(chunk_duration * 1000)
        num_chunks = (duration_ms // chunk_length_ms) + (1 if duration_ms % chunk_length_ms > 0 else 0)
        
        progress(0.1, desc=f"{num_chunks} ‡§ö‡§Ç‡§ï‡•ç‡§∏ ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏ ‡§π‡•ã ‡§∞‡§π‡•á ‡§π‡•à‡§Ç... / Processing {num_chunks} chunks...")
        
        recognizer = sr.Recognizer()
        recognizer.energy_threshold = 300
        recognizer.dynamic_energy_threshold = True
        
        lang_code = "hi-IN" if language == "Hindi (‡§π‡§ø‡§Ç‡§¶‡•Ä)" else "en-US"
        
        transcriptions = []
        successful_chunks = 0
        failed_chunks = 0
        filtered_out = 0
        chunk_stats = []
        
        for i in range(num_chunks):
            start_ms = i * chunk_length_ms
            end_ms = min((i + 1) * chunk_length_ms, duration_ms)
            
            chunk = audio[start_ms:end_ms]
            chunk_duration_sec = len(chunk) / 1000.0
            
            if chunk_duration_sec < 1.0:
                continue
            
            chunk_filename = f"chunk_{i:04d}.wav"
            chunk_path = os.path.join(audio_chunks_dir, chunk_filename)
            chunk.export(chunk_path, format="wav")
            
            y, sr_rate = librosa.load(chunk_path, sr=22050)
            
            if apply_noise_reduction:
                y = apply_spectral_gating(y, sr_rate)
            
            if remove_silence:
                y_trimmed, _ = librosa.effects.trim(y, top_db=20)
                if len(y_trimmed) > 0:
                    y = y_trimmed
            
            rms_energy = float(np.sqrt(np.mean(y**2)))
            zero_crossing_rate = float(np.mean(librosa.feature.zero_crossing_rate(y)))
            spectral_centroid = float(np.mean(librosa.feature.spectral_centroid(y=y, sr=sr_rate)))
            
            sf.write(chunk_path, y, sr_rate)
            
            actual_duration = len(y) / sr_rate
            if actual_duration < min_duration or actual_duration > max_duration:
                filtered_out += 1
                continue
            
            try:
                with sr.AudioFile(chunk_path) as source:
                    audio_data = recognizer.record(source)
                    text = recognizer.recognize_google(audio_data, language=lang_code)
                    
                    if text.strip():
                        word_count = len(text.split())
                        
                        filtered_chunk_path = os.path.join(filtered_chunks_dir, chunk_filename)
                        shutil.copy(chunk_path, filtered_chunk_path)
                        
                        transcriptions.append(f"{chunk_filename}|{text}\n")
                        successful_chunks += 1
                        
                        chunk_stats.append({
                            "filename": chunk_filename,
                            "duration": actual_duration,
                            "transcription": text,
                            "word_count": word_count,
                            "rms_energy": rms_energy,
                            "zero_crossing_rate": zero_crossing_rate,
                            "spectral_centroid": spectral_centroid
                        })
                    else:
                        failed_chunks += 1
                        
            except sr.UnknownValueError:
                transcriptions.append(f"{chunk_filename}|[UNINTELLIGIBLE]\n")
                failed_chunks += 1
            except sr.RequestError as e:
                transcriptions.append(f"{chunk_filename}|[ERROR: {str(e)}]\n")
                failed_chunks += 1
            except Exception as e:
                transcriptions.append(f"{chunk_filename}|[ERROR: {str(e)}]\n")
                failed_chunks += 1
            
            progress((i + 1) / num_chunks, desc=f"‡§ö‡§Ç‡§ï {i + 1}/{num_chunks} ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏ ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•à... / Processing chunk {i + 1}/{num_chunks}")
        
        transcript_file = os.path.join(output_dir, "transcriptions.txt")
        with open(transcript_file, "w", encoding="utf-8") as f:
            f.writelines(transcriptions)
        
        metadata_file = os.path.join(output_dir, "metadata.csv")
        with open(metadata_file, "w", encoding="utf-8") as f:
            for line in transcriptions:
                if "|" in line and "[" not in line:
                    f.write(line)
        
        stats_file = os.path.join(stats_dir, "audio_statistics.json")
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(chunk_stats, f, indent=2, ensure_ascii=False)
        
        analysis_file = generate_analysis_report(chunk_stats, stats_dir)
        
        summary = f"""
‚úÖ ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏‡§ø‡§Ç‡§ó ‡§™‡•Ç‡§∞‡•Ä ‡§π‡•Å‡§à! / Processing Complete!

üìä ‡§∏‡§æ‡§Ç‡§ñ‡•ç‡§Ø‡§ø‡§ï‡•Ä / Statistics:
- ‡§ï‡•Å‡§≤ ‡§ö‡§Ç‡§ï‡•ç‡§∏ ‡§¨‡§®‡§æ‡§è ‡§ó‡§è / Total chunks created: {num_chunks}
- ‡§∏‡§´‡§≤‡§§‡§æ‡§™‡•Ç‡§∞‡•ç‡§µ‡§ï ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§ï‡•ç‡§∞‡§æ‡§á‡§¨ ‡§ï‡§ø‡§è ‡§ó‡§è / Successfully transcribed: {successful_chunks}
- ‡§Ö‡§∏‡§´‡§≤/‡§Ö‡§∏‡•ç‡§™‡§∑‡•ç‡§ü / Failed/Unintelligible: {failed_chunks}
- ‡§´‡§º‡§ø‡§≤‡•ç‡§ü‡§∞ ‡§ï‡§ø‡§è ‡§ó‡§è (‡§Ö‡§µ‡§ß‡§ø) / Filtered out (duration): {filtered_out}
- ‡§ï‡•Å‡§≤ ‡§ë‡§°‡§ø‡§Ø‡•ã ‡§Ö‡§µ‡§ß‡§ø / Total audio duration: {duration_ms / 1000:.2f} seconds
- ‡§î‡§∏‡§§ ‡§ö‡§Ç‡§ï ‡§Ö‡§µ‡§ß‡§ø / Average chunk duration: {np.mean([s['duration'] for s in chunk_stats]):.2f}s (if chunk_stats else 0)

üìÅ ‡§Ü‡§â‡§ü‡§™‡•Å‡§ü ‡§∏‡•ç‡§•‡§æ‡§® / Output Location:
- ‡§∏‡§≠‡•Ä ‡§ë‡§°‡§ø‡§Ø‡•ã ‡§ö‡§Ç‡§ï‡•ç‡§∏ / All audio chunks: {audio_chunks_dir}
- ‡§´‡§º‡§ø‡§≤‡•ç‡§ü‡§∞ ‡§ï‡§ø‡§è ‡§ó‡§è ‡§ö‡§Ç‡§ï‡•ç‡§∏ / Filtered chunks: {filtered_chunks_dir}
- ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§∂‡§® / Transcriptions: {transcript_file}
- ‡§Æ‡•á‡§ü‡§æ‡§°‡•á‡§ü‡§æ (‡§ü‡•ç‡§∞‡•á‡§®‡§ø‡§Ç‡§ó ‡§ï‡•á ‡§≤‡§ø‡§è) / Metadata (for training): {metadata_file}

üí° ‡§´‡§º‡§æ‡§á‡§≤‡•á‡§Ç TTS ‡§Æ‡•â‡§°‡§≤ ‡§ü‡•ç‡§∞‡•á‡§®‡§ø‡§Ç‡§ó ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡§Ç! / Files are ready for TTS model training!
        """
        
        return summary, transcript_file, metadata_file, stats_file
        
    except Exception as e:
        return f"‚ùå ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø / Error: {str(e)}", None, None, None


def apply_spectral_gating(audio, sr, n_fft=2048, hop_length=512):
    """Simple noise reduction using spectral gating"""
    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)
    magnitude = np.abs(stft)
    noise_floor = np.percentile(magnitude, 10, axis=1, keepdims=True)
    mask = magnitude > (noise_floor * 1.5)
    stft_denoised = stft * mask
    audio_denoised = librosa.istft(stft_denoised, hop_length=hop_length)
    return audio_denoised


def generate_analysis_report(chunk_stats, stats_dir):
    """Generate detailed analysis report"""
    if not chunk_stats:
        return None
    
    report_file = os.path.join(stats_dir, "analysis_report.txt")
    
    durations = [s['duration'] for s in chunk_stats]
    word_counts = [s['word_count'] for s in chunk_stats]
    energies = [s['rms_energy'] for s in chunk_stats]
    
    report = f"""
üîç ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü / Detailed Analysis Report
{'='*60}

üìä ‡§Ö‡§µ‡§ß‡§ø ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ / Duration Analysis:
- ‡§î‡§∏‡§§ / Average: {np.mean(durations):.2f}s
- ‡§®‡•ç‡§Ø‡•Ç‡§®‡§§‡§Æ / Minimum: {np.min(durations):.2f}s
- ‡§Ö‡§ß‡§ø‡§ï‡§§‡§Æ / Maximum: {np.max(durations):.2f}s
- ‡§Æ‡§æ‡§®‡§ï ‡§µ‡§ø‡§ö‡§≤‡§® / Std Dev: {np.std(durations):.2f}s

üìù ‡§∂‡§¨‡•ç‡§¶ ‡§ó‡§£‡§®‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ / Word Count Analysis:
- ‡§î‡§∏‡§§ ‡§∂‡§¨‡•ç‡§¶ ‡§™‡•ç‡§∞‡§§‡§ø ‡§ö‡§Ç‡§ï / Average words per chunk: {np.mean(word_counts):.1f}
- ‡§®‡•ç‡§Ø‡•Ç‡§®‡§§‡§Æ / Minimum: {np.min(word_counts)}
- ‡§Ö‡§ß‡§ø‡§ï‡§§‡§Æ / Maximum: {np.max(word_counts)}

üîä ‡§ë‡§°‡§ø‡§Ø‡•ã ‡§ó‡•Å‡§£‡§µ‡§§‡•ç‡§§‡§æ / Audio Quality:
- ‡§î‡§∏‡§§ RMS ‡§ä‡§∞‡•ç‡§ú‡§æ / Average RMS Energy: {np.mean(energies):.4f}
- ‡§ä‡§∞‡•ç‡§ú‡§æ ‡§∏‡•ç‡§•‡§ø‡§∞‡§§‡§æ / Energy Consistency: {'‡§â‡§ö‡•ç‡§ö / High' if np.std(energies) < 0.01 else '‡§Æ‡§ß‡•ç‡§Ø‡§Æ / Medium'}

üí° ‡§∏‡§ø‡§´‡§æ‡§∞‡§ø‡§∂‡•á‡§Ç / Recommendations:
{'- ‡§Ö‡§ö‡•ç‡§õ‡•Ä ‡§ó‡•Å‡§£‡§µ‡§§‡•ç‡§§‡§æ, ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞ / Good quality, ready for training' if np.mean(energies) > 0.01 else '- ‡§ï‡§Æ ‡§ä‡§∞‡•ç‡§ú‡§æ ‡§ï‡§æ ‡§™‡§§‡§æ ‡§ö‡§≤‡§æ, ‡§µ‡•â‡§≤‡•ç‡§Ø‡•Ç‡§Æ ‡§¨‡§¢‡§º‡§æ‡§®‡•á ‡§™‡§∞ ‡§µ‡§ø‡§ö‡§æ‡§∞ ‡§ï‡§∞‡•á‡§Ç / Low energy detected, consider increasing volume'}
"""
    
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(report)
    
    return report_file


# Function 1: Validate Dataset
def validate_dataset(progress=gr.Progress()):
    """Validate the dataset for TTS training"""
    output_dir = "tts_training_data"
    metadata_file = os.path.join(output_dir, "metadata.csv")
    
    if not os.path.exists(metadata_file):
        return "‚ùå ‡§ï‡•ã‡§à ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ! ‡§™‡§π‡§≤‡•á ‡§ë‡§°‡§ø‡§Ø‡•ã ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏ ‡§ï‡§∞‡•á‡§Ç‡•§ / No dataset found! Process audio first."
    
    progress(0, desc="‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•à... / Validating dataset...")
    
    issues = []
    valid_count = 0
    
    with open(metadata_file, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    for i, line in enumerate(lines):
        if "|" in line:
            filename, text = line.strip().split("|", 1)
            audio_path = os.path.join(output_dir, "filtered_chunks", filename)
            
            if not os.path.exists(audio_path):
                issues.append(f"‚ùå ‡§≤‡§æ‡§á‡§® {i+1}: ‡§ë‡§°‡§ø‡§Ø‡•ã ‡§´‡§º‡§æ‡§á‡§≤ ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•Ä / Line {i+1}: Audio file not found - {filename}")
            elif not text.strip():
                issues.append(f"‚ö†Ô∏è ‡§≤‡§æ‡§á‡§® {i+1}: ‡§ñ‡§æ‡§≤‡•Ä ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§∂‡§® / Line {i+1}: Empty transcription - {filename}")
            elif len(text.split()) < 2:
                issues.append(f"‚ö†Ô∏è ‡§≤‡§æ‡§á‡§® {i+1}: ‡§¨‡§π‡•Å‡§§ ‡§õ‡•ã‡§ü‡§æ ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§∂‡§® / Line {i+1}: Very short transcription - {filename}")
            else:
                valid_count += 1
        
        progress((i + 1) / len(lines))
    
    report = f"""
‚úÖ ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§Æ‡§æ‡§®‡•ç‡§Ø‡§§‡§æ ‡§™‡•Ç‡§∞‡•ç‡§£ / Dataset Validation Complete!

üìä ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results:
- ‡§ï‡•Å‡§≤ ‡§™‡•ç‡§∞‡§µ‡§ø‡§∑‡•ç‡§ü‡§ø‡§Ø‡§æ‡§Å / Total entries: {len(lines)}
- ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§™‡•ç‡§∞‡§µ‡§ø‡§∑‡•ç‡§ü‡§ø‡§Ø‡§æ‡§Å / Valid entries: {valid_count}
- ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ‡§è‡§Å ‡§Æ‡§ø‡§≤‡•Ä‡§Ç / Issues found: {len(issues)}

{'üéâ ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à! / Dataset is ready for training!' if len(issues) == 0 else '‚ö†Ô∏è ‡§ï‡•Å‡§õ ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ‡§è‡§Å ‡§Æ‡§ø‡§≤‡•Ä‡§Ç / Some issues found:'}

{chr(10).join(issues[:10])}
{'...(‡§î‡§∞ ‡§Ö‡§ß‡§ø‡§ï / and more)' if len(issues) > 10 else ''}
    """
    
    return report


# Function 2: Balance Dataset
def balance_dataset(min_samples, progress=gr.Progress()):
    """Balance dataset by removing over-represented samples"""
    output_dir = "tts_training_data"
    metadata_file = os.path.join(output_dir, "metadata.csv")
    
    if not os.path.exists(metadata_file):
        return "‚ùå ‡§ï‡•ã‡§à ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ! / No dataset found!"
    
    progress(0, desc="‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§∏‡§Ç‡§§‡•Å‡§≤‡§ø‡§§ ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•à... / Balancing dataset...")
    
    with open(metadata_file, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    word_freq = {}
    entries = []
    
    for line in lines:
        if "|" in line:
            filename, text = line.strip().split("|", 1)
            words = text.lower().split()
            entries.append((filename, text, words))
            
            for word in words:
                word_freq[word] = word_freq.get(word, 0) + 1
    
    balanced_entries = []
    word_usage = {}
    
    for filename, text, words in entries:
        should_add = True
        for word in words:
            if word_usage.get(word, 0) >= min_samples:
                should_add = False
                break
        
        if should_add:
            balanced_entries.append(f"{filename}|{text}\n")
            for word in words:
                word_usage[word] = word_usage.get(word, 0) + 1
    
    balanced_file = os.path.join(output_dir, "metadata_balanced.csv")
    with open(balanced_file, "w", encoding="utf-8") as f:
        f.writelines(balanced_entries)
    
    progress(1.0)
    
    report = f"""
‚úÖ ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§∏‡§Ç‡§§‡•Å‡§≤‡§® ‡§™‡•Ç‡§∞‡•ç‡§£ / Dataset Balancing Complete!

üìä ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results:
- ‡§Æ‡•Ç‡§≤ ‡§™‡•ç‡§∞‡§µ‡§ø‡§∑‡•ç‡§ü‡§ø‡§Ø‡§æ‡§Å / Original entries: {len(entries)}
- ‡§∏‡§Ç‡§§‡•Å‡§≤‡§ø‡§§ ‡§™‡•ç‡§∞‡§µ‡§ø‡§∑‡•ç‡§ü‡§ø‡§Ø‡§æ‡§Å / Balanced entries: {len(balanced_entries)}
- ‡§π‡§ü‡§æ‡§à ‡§ó‡§à / Removed: {len(entries) - len(balanced_entries)}

üìÅ ‡§∏‡§Ç‡§§‡•Å‡§≤‡§ø‡§§ ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§∏‡§π‡•á‡§ú‡§æ ‡§ó‡§Ø‡§æ / Balanced dataset saved: {balanced_file}
    """
    
    return report


# Function 3: Train/Val Split
def export_for_training(train_split, progress=gr.Progress()):
    """Split dataset into train and validation sets"""
    output_dir = "tts_training_data"
    metadata_file = os.path.join(output_dir, "metadata.csv")
    
    if not os.path.exists(metadata_file):
        return "‚ùå ‡§ï‡•ã‡§à ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ! / No dataset found!", None, None
    
    progress(0, desc="‡§ü‡•ç‡§∞‡•á‡§®‡§ø‡§Ç‡§ó ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§ï‡•ç‡§∏‡§™‡•ã‡§∞‡•ç‡§ü ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•à... / Exporting for training...")
    
    with open(metadata_file, "r", encoding="utf-8") as f:
        lines = [l for l in f.readlines() if "|" in l and "[" not in l]
    
    np.random.shuffle(lines)
    
    split_idx = int(len(lines) * train_split / 100)
    train_lines = lines[:split_idx]
    val_lines = lines[split_idx:]
    
    train_file = os.path.join(output_dir, "train.txt")
    with open(train_file, "w", encoding="utf-8") as f:
        f.writelines(train_lines)
    
    val_file = os.path.join(output_dir, "val.txt")
    with open(val_file, "w", encoding="utf-8") as f:
        f.writelines(val_lines)
    
    progress(1.0)
    
    summary = f"""
‚úÖ ‡§è‡§ï‡•ç‡§∏‡§™‡•ã‡§∞‡•ç‡§ü ‡§™‡•Ç‡§∞‡•ç‡§£ / Export Complete!

üìä ‡§µ‡§ø‡§≠‡§æ‡§ú‡§® / Split:
- ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§∏‡•á‡§ü / Training set: {len(train_lines)} samples
- ‡§∏‡§§‡•ç‡§Ø‡§æ‡§™‡§® ‡§∏‡•á‡§ü / Validation set: {len(val_lines)} samples
- ‡§Ö‡§®‡•Å‡§™‡§æ‡§§ / Ratio: {train_split}% / {100-train_split}%

üìÅ ‡§´‡§º‡§æ‡§á‡§≤‡•á‡§Ç / Files:
- {train_file}
- {val_file}

üöÄ TTS ‡§Æ‡•â‡§°‡§≤ ‡§ü‡•ç‡§∞‡•á‡§®‡§ø‡§Ç‡§ó ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞! / Ready to start TTS model training!
    """
    
    return summary, train_file, val_file


# Function 4: Duplicate Detection
def detect_duplicates(progress=gr.Progress()):
    """Detect duplicate transcriptions"""
    output_dir = "tts_training_data"
    metadata_file = os.path.join(output_dir, "metadata.csv")
    
    if not os.path.exists(metadata_file):
        return "‚ùå ‡§ï‡•ã‡§à ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ! / No dataset found!"
    
    progress(0, desc="‡§°‡•Å‡§™‡•ç‡§≤‡•Ä‡§ï‡•á‡§ü ‡§ï‡§æ ‡§™‡§§‡§æ ‡§≤‡§ó‡§æ‡§Ø‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à... / Detecting duplicates...")
    
    with open(metadata_file, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    transcriptions = {}
    duplicates = []
    
    for line in lines:
        if "|" in line:
            filename, text = line.strip().split("|", 1)
            if text in transcriptions:
                duplicates.append(f"'{text}' - Files: {transcriptions[text]}, {filename}")
            else:
                transcriptions[text] = filename
    
    progress(1.0)
    
    report = f"""
‚úÖ ‡§°‡•Å‡§™‡•ç‡§≤‡•Ä‡§ï‡•á‡§ü ‡§ú‡§æ‡§Ç‡§ö ‡§™‡•Ç‡§∞‡•ç‡§£ / Duplicate Check Complete!

üìä ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results:
- ‡§ï‡•Å‡§≤ ‡§™‡•ç‡§∞‡§µ‡§ø‡§∑‡•ç‡§ü‡§ø‡§Ø‡§æ‡§Å / Total entries: {len(lines)}
- ‡§Ø‡•Ç‡§®‡§ø‡§ï ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§∂‡§® / Unique transcriptions: {len(transcriptions)}
- ‡§°‡•Å‡§™‡•ç‡§≤‡•Ä‡§ï‡•á‡§ü ‡§Æ‡§ø‡§≤‡•á / Duplicates found: {len(duplicates)}

{chr(10).join(duplicates[:15]) if duplicates else 'üéâ ‡§ï‡•ã‡§à ‡§°‡•Å‡§™‡•ç‡§≤‡•Ä‡§ï‡•á‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ! / No duplicates found!'}
{'...(‡§î‡§∞ ‡§Ö‡§ß‡§ø‡§ï / and more)' if len(duplicates) > 15 else ''}
    """
    
    return report


# Function 5: Character Count Analysis
def character_count_analysis(progress=gr.Progress()):
    """Analyze character count distribution"""
    output_dir = "tts_training_data"
    metadata_file = os.path.join(output_dir, "metadata.csv")
    
    if not os.path.exists(metadata_file):
        return "‚ùå ‡§ï‡•ã‡§à ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ! / No dataset found!"
    
    progress(0, desc="‡§µ‡§∞‡•ç‡§£ ‡§ó‡§£‡§®‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£... / Character count analysis...")
    
    with open(metadata_file, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    char_counts = []
    for line in lines:
        if "|" in line:
            _, text = line.strip().split("|", 1)
            char_counts.append(len(text))
    
    progress(1.0)
    
    report = f"""
‚úÖ ‡§µ‡§∞‡•ç‡§£ ‡§ó‡§£‡§®‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§™‡•Ç‡§∞‡•ç‡§£ / Character Count Analysis Complete!

üìä ‡§∏‡§æ‡§Ç‡§ñ‡•ç‡§Ø‡§ø‡§ï‡•Ä / Statistics:
- ‡§î‡§∏‡§§ ‡§µ‡§∞‡•ç‡§£ / Average characters: {np.mean(char_counts):.1f}
- ‡§®‡•ç‡§Ø‡•Ç‡§®‡§§‡§Æ / Minimum: {np.min(char_counts)}
- ‡§Ö‡§ß‡§ø‡§ï‡§§‡§Æ / Maximum: {np.max(char_counts)}
- ‡§Æ‡§æ‡§®‡§ï ‡§µ‡§ø‡§ö‡§≤‡§® / Std Dev: {np.std(char_counts):.1f}
- ‡§Æ‡§ß‡•ç‡§Ø‡§ø‡§ï‡§æ / Median: {np.median(char_counts):.1f}

üí° ‡§∏‡§ø‡§´‡§æ‡§∞‡§ø‡§∂ / Recommendation:
{'‚úÖ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§µ‡§ø‡§§‡§∞‡§£ / Good distribution' if 50 < np.mean(char_counts) < 200 else '‚ö†Ô∏è ‡§µ‡§ø‡§§‡§∞‡§£ ‡§ï‡•Ä ‡§ú‡§æ‡§Ç‡§ö ‡§ï‡§∞‡•á‡§Ç / Check distribution'}
    """
    
    return report


# Function 6: Word Frequency Analysis
def word_frequency_analysis(top_n, progress=gr.Progress()):
    """Analyze most common words"""
    output_dir = "tts_training_data"
    metadata_file = os.path.join(output_dir, "metadata.csv")
    
    if not os.path.exists(metadata_file):
        return "‚ùå ‡§ï‡•ã‡§à ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ! / No dataset found!"
    
    progress(0, desc="‡§∂‡§¨‡•ç‡§¶ ‡§Ü‡§µ‡•É‡§§‡•ç‡§§‡§ø ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£... / Word frequency analysis...")
    
    with open(metadata_file, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    all_words = []
    for line in lines:
        if "|" in line:
            _, text = line.strip().split("|", 1)
            all_words.extend(text.lower().split())
    
    word_freq = Counter(all_words)
    most_common = word_freq.most_common(top_n)
    
    progress(1.0)
    
    report = f"""
‚úÖ ‡§∂‡§¨‡•ç‡§¶ ‡§Ü‡§µ‡•É‡§§‡•ç‡§§‡§ø ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§™‡•Ç‡§∞‡•ç‡§£ / Word Frequency Analysis Complete!

üìä ‡§∂‡•Ä‡§∞‡•ç‡§∑ {top_n} ‡§∂‡§¨‡•ç‡§¶ / Top {top_n} Words:
{chr(10).join([f"{i+1}. '{word}' - {count} ‡§¨‡§æ‡§∞ / times" for i, (word, count) in enumerate(most_common)])}

üí° ‡§ï‡•Å‡§≤ ‡§Ø‡•Ç‡§®‡§ø‡§ï ‡§∂‡§¨‡•ç‡§¶ / Total unique words: {len(word_freq)}
    """
    
    return report


# Function 7: Remove Short Samples
def remove_short_samples(min_words, progress=gr.Progress()):
    """Remove samples with fewer than minimum words"""
    output_dir = "tts_training_data"
    metadata_file = os.path.join(output_dir, "metadata.csv")
    
    if not os.path.exists(metadata_file):
        return "‚ùå ‡§ï‡•ã‡§à ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ! / No dataset found!"
    
    progress(0, desc="‡§õ‡•ã‡§ü‡•á ‡§®‡§Æ‡•Ç‡§®‡•á ‡§π‡§ü‡§æ‡§è ‡§ú‡§æ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç... / Removing short samples...")
    
    with open(metadata_file, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    filtered_lines = []
    removed_count = 0
    
    for line in lines:
        if "|" in line:
            filename, text = line.strip().split("|", 1)
            if len(text.split()) >= min_words:
                filtered_lines.append(line)
            else:
                removed_count += 1
    
    filtered_file = os.path.join(output_dir, "metadata_filtered.csv")
    with open(filtered_file, "w", encoding="utf-8") as f:
        f.writelines(filtered_lines)
    
    progress(1.0)
    
    report = f"""
‚úÖ ‡§´‡§º‡§ø‡§≤‡•ç‡§ü‡§∞‡§ø‡§Ç‡§ó ‡§™‡•Ç‡§∞‡•ç‡§£ / Filtering Complete!

üìä ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results:
- ‡§Æ‡•Ç‡§≤ ‡§®‡§Æ‡•Ç‡§®‡•á / Original samples: {len(lines)}
- ‡§´‡§º‡§ø‡§≤‡•ç‡§ü‡§∞ ‡§ï‡§ø‡§è ‡§ó‡§è ‡§®‡§Æ‡•Ç‡§®‡•á / Filtered samples: {len(filtered_lines)}
- ‡§π‡§ü‡§æ‡§è ‡§ó‡§è / Removed: {removed_count}

üìÅ ‡§´‡§º‡§æ‡§á‡§≤ ‡§∏‡§π‡•á‡§ú‡•Ä ‡§ó‡§à / File saved: {filtered_file}
    """
    
    return report


# Function 8: Generate Dataset Report
def generate_dataset_report(progress=gr.Progress()):
    """Generate comprehensive dataset report"""
    output_dir = "tts_training_data"
    metadata_file = os.path.join(output_dir, "metadata.csv")
    
    if not os.path.exists(metadata_file):
        return "‚ùå ‡§ï‡•ã‡§à ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ! / No dataset found!"
    
    progress(0, desc="‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü ‡§ú‡§®‡§∞‡•á‡§ü ‡§π‡•ã ‡§∞‡§π‡•Ä ‡§π‡•à... / Generating report...")
    
    with open(metadata_file, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    total_samples = len(lines)
    total_words = 0
    total_chars = 0
    durations = []
    
    for line in lines:
        if "|" in line:
            filename, text = line.strip().split("|", 1)
            total_words += len(text.split())
            total_chars += len(text)
    
    progress(0.5)
    
    report_file = os.path.join(output_dir, "dataset_report.txt")
    report_content = f"""
{'='*70}
üìä ‡§∏‡§Ç‡§™‡•Ç‡§∞‡•ç‡§£ ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü / COMPREHENSIVE DATASET REPORT
{'='*70}
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

üìà BASIC STATISTICS / ‡§¨‡•Å‡§®‡§ø‡§Ø‡§æ‡§¶‡•Ä ‡§Ü‡§Å‡§ï‡§°‡§º‡•á:
- Total Samples / ‡§ï‡•Å‡§≤ ‡§®‡§Æ‡•Ç‡§®‡•á: {total_samples}
- Total Words / ‡§ï‡•Å‡§≤ ‡§∂‡§¨‡•ç‡§¶: {total_words}
- Total Characters / ‡§ï‡•Å‡§≤ ‡§µ‡§∞‡•ç‡§£: {total_chars}
- Avg Words per Sample / ‡§î‡§∏‡§§ ‡§∂‡§¨‡•ç‡§¶ ‡§™‡•ç‡§∞‡§§‡§ø ‡§®‡§Æ‡•Ç‡§®‡§æ: {total_words/total_samples if total_samples > 0 else 0:.1f}
- Avg Characters per Sample / ‡§î‡§∏‡§§ ‡§µ‡§∞‡•ç‡§£ ‡§™‡•ç‡§∞‡§§‡§ø ‡§®‡§Æ‡•Ç‡§®‡§æ: {total_chars/total_samples if total_samples > 0 else 0:.1f}

üéØ DATASET READINESS / ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§§‡•à‡§Ø‡§æ‡§∞‡•Ä:
‚úÖ Ready for TTS Training / TTS ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞

üìÅ OUTPUT FILES / ‡§Ü‡§â‡§ü‡§™‡•Å‡§ü ‡§´‡§º‡§æ‡§á‡§≤‡•á‡§Ç:
- Metadata: {metadata_file}
- Audio Chunks: {os.path.join(output_dir, 'filtered_chunks')}

üí° RECOMMENDATIONS / ‡§∏‡§ø‡§´‡§æ‡§∞‡§ø‡§∂‡•á‡§Ç:
- Validate dataset before training / ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§∏‡•á ‡§™‡§π‡§≤‡•á ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ï‡§∞‡•á‡§Ç
- Split into train/val sets / ‡§ü‡•ç‡§∞‡•á‡§®/‡§µ‡•à‡§≤ ‡§∏‡•á‡§ü ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§≠‡§æ‡§ú‡§ø‡§§ ‡§ï‡§∞‡•á‡§Ç
- Check for duplicates / ‡§°‡•Å‡§™‡•ç‡§≤‡•Ä‡§ï‡•á‡§ü ‡§ï‡•Ä ‡§ú‡§æ‡§Ç‡§ö ‡§ï‡§∞‡•á‡§Ç

{'='*70}
Report saved to: {report_file}
{'='*70}
    """
    
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(report_content)
    
    progress(1.0)
    
    return report_content


# Function 9: Merge Multiple Datasets
def merge_datasets(progress=gr.Progress()):
    """Merge multiple metadata files"""
    output_dir = "tts_training_data"
    
    metadata_files = [
        os.path.join(output_dir, "metadata.csv"),
        os.path.join(output_dir, "metadata_balanced.csv"),
        os.path.join(output_dir, "metadata_filtered.csv")
    ]
    
    existing_files = [f for f in metadata_files if os.path.exists(f)]
    
    if len(existing_files) < 2:
        return "‚ùå ‡§ï‡§Æ ‡§∏‡•á ‡§ï‡§Æ 2 ‡§Æ‡•á‡§ü‡§æ‡§°‡•á‡§ü‡§æ ‡§´‡§º‡§æ‡§á‡§≤‡•á‡§Ç ‡§ö‡§æ‡§π‡§ø‡§è / Need at least 2 metadata files!"
    
    progress(0, desc="‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§Æ‡§∞‡•ç‡§ú ‡§π‡•ã ‡§∞‡§π‡•á ‡§π‡•à‡§Ç... / Merging datasets...")
    
    all_lines = []
    for file in existing_files:
        with open(file, "r", encoding="utf-8") as f:
            all_lines.extend(f.readlines())
    
    # Remove duplicates
    unique_lines = list(set(all_lines))
    
    merged_file = os.path.join(output_dir, "metadata_merged.csv")
    with open(merged_file, "w", encoding="utf-8") as f:
        f.writelines(unique_lines)
    
    progress(1.0)
    
    report = f"""
‚úÖ ‡§Æ‡§∞‡•ç‡§ú ‡§™‡•Ç‡§∞‡•ç‡§£ / Merge Complete!

üìä ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results:
- ‡§Æ‡§∞‡•ç‡§ú ‡§ï‡•Ä ‡§ó‡§à ‡§´‡§º‡§æ‡§á‡§≤‡•á‡§Ç / Merged files: {len(existing_files)}
- ‡§ï‡•Å‡§≤ ‡§≤‡§æ‡§á‡§®‡•á‡§Ç / Total lines: {len(all_lines)}
- ‡§Ø‡•Ç‡§®‡§ø‡§ï ‡§≤‡§æ‡§á‡§®‡•á‡§Ç / Unique lines: {len(unique_lines)}
- ‡§°‡•Å‡§™‡•ç‡§≤‡•Ä‡§ï‡•á‡§ü ‡§π‡§ü‡§æ‡§è ‡§ó‡§è / Duplicates removed: {len(all_lines) - len(unique_lines)}

üìÅ ‡§Æ‡§∞‡•ç‡§ú ‡§ï‡•Ä ‡§ó‡§à ‡§´‡§º‡§æ‡§á‡§≤ / Merged file: {merged_file}
    """
    
    return report


# Function 10: Export to JSON Format
def export_to_json(progress=gr.Progress()):
    """Export metadata to JSON format"""
    output_dir = "tts_training_data"
    metadata_file = os.path.join(output_dir, "metadata.csv")
    
    if not os.path.exists(metadata_file):
        return "‚ùå ‡§ï‡•ã‡§à ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ! / No dataset found!"
    
    progress(0, desc="JSON ‡§Æ‡•á‡§Ç ‡§è‡§ï‡•ç‡§∏‡§™‡•ã‡§∞‡•ç‡§ü ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•à... / Exporting to JSON...")
    
    with open(metadata_file, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    json_data = []
    for line in lines:
        if "|" in line:
            filename, text = line.strip().split("|", 1)
            json_data.append({
                "audio_file": filename,
                "transcription": text,
                "word_count": len(text.split()),
                "char_count": len(text)
            })
    
    json_file = os.path.join(output_dir, "metadata.json")
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)
    
    progress(1.0)
    
    return f"""
‚úÖ JSON ‡§è‡§ï‡•ç‡§∏‡§™‡•ã‡§∞‡•ç‡§ü ‡§™‡•Ç‡§∞‡•ç‡§£ / JSON Export Complete!

üìä ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results:
- ‡§ï‡•Å‡§≤ ‡§™‡•ç‡§∞‡§µ‡§ø‡§∑‡•ç‡§ü‡§ø‡§Ø‡§æ‡§Å / Total entries: {len(json_data)}
- ‡§´‡§º‡§æ‡§á‡§≤ ‡§ï‡§æ ‡§Ü‡§ï‡§æ‡§∞ / File size: {os.path.getsize(json_file) / 1024:.2f} KB

üìÅ JSON ‡§´‡§º‡§æ‡§á‡§≤ / JSON file: {json_file}
    """


# Function 11: Sample Random Entries
def sample_random_entries(num_samples, progress=gr.Progress()):
    """Sample random entries from dataset"""
    output_dir = "tts_training_data"
    metadata_file = os.path.join(output_dir, "metadata.csv")
    
    if not os.path.exists(metadata_file):
        return "‚ùå ‡§ï‡•ã‡§à ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ! / No dataset found!"
    
    progress(0, desc="‡§∞‡•à‡§Ç‡§°‡§Æ ‡§∏‡•à‡§Ç‡§™‡§≤ ‡§®‡§ø‡§ï‡§æ‡§≤‡•á ‡§ú‡§æ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç... / Sampling random entries...")
    
    with open(metadata_file, "r", encoding="utf-8") as f:
        lines = [l for l in f.readlines() if "|" in l]
    
    if len(lines) < num_samples:
        num_samples = len(lines)
    
    sampled = np.random.choice(lines, num_samples, replace=False)
    
    sample_file = os.path.join(output_dir, "sample_dataset.csv")
    with open(sample_file, "w", encoding="utf-8") as f:
        f.writelines(sampled)
    
    progress(1.0)
    
    preview = "\n".join([f"{i+1}. {line.strip()}" for i, line in enumerate(sampled[:5])])
    
    return f"""
‚úÖ ‡§∏‡•à‡§Ç‡§™‡§≤‡§ø‡§Ç‡§ó ‡§™‡•Ç‡§∞‡•ç‡§£ / Sampling Complete!

üìä ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results:
- ‡§∏‡•à‡§Ç‡§™‡§≤ ‡§ï‡§ø‡§è ‡§ó‡§è / Sampled: {num_samples} entries

üìù ‡§™‡§π‡§≤‡•á 5 ‡§è‡§Ç‡§ü‡•ç‡§∞‡•Ä‡§ú‡§º / First 5 Entries:
{preview}

üìÅ ‡§∏‡•à‡§Ç‡§™‡§≤ ‡§´‡§º‡§æ‡§á‡§≤ / Sample file: {sample_file}
    """


# Function 12: Calculate Dataset Size
def calculate_dataset_size(progress=gr.Progress()):
    """Calculate total dataset size"""
    output_dir = "tts_training_data"
    
    if not os.path.exists(output_dir):
        return "‚ùå ‡§ï‡•ã‡§à ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ! / No dataset found!"
    
    progress(0, desc="‡§∏‡§æ‡§á‡§ú‡§º ‡§ï‡•Ä ‡§ó‡§£‡§®‡§æ ‡§π‡•ã ‡§∞‡§π‡•Ä ‡§π‡•à... / Calculating size...")
    
    total_size = 0
    file_count = 0
    
    for root, dirs, files in os.walk(output_dir):
        for file in files:
            file_path = os.path.join(root, file)
            total_size += os.path.getsize(file_path)
            file_count += 1
    
    progress(1.0)
    
    size_mb = total_size / (1024 * 1024)
    size_gb = total_size / (1024 * 1024 * 1024)
    
    return f"""
‚úÖ ‡§∏‡§æ‡§á‡§ú‡§º ‡§ó‡§£‡§®‡§æ ‡§™‡•Ç‡§∞‡•ç‡§£ / Size Calculation Complete!

üìä ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results:
- ‡§ï‡•Å‡§≤ ‡§´‡§º‡§æ‡§á‡§≤‡•á‡§Ç / Total files: {file_count}
- ‡§ï‡•Å‡§≤ ‡§∏‡§æ‡§á‡§ú‡§º / Total size: {size_mb:.2f} MB ({size_gb:.3f} GB)
- ‡§î‡§∏‡§§ ‡§´‡§º‡§æ‡§á‡§≤ ‡§∏‡§æ‡§á‡§ú‡§º / Avg file size: {size_mb/file_count if file_count > 0 else 0:.2f} MB

üíæ ‡§°‡§ø‡§∏‡•ç‡§ï ‡§∏‡•ç‡§™‡•á‡§∏ / Disk Space: {'‚úÖ ‡§Ö‡§ö‡•ç‡§õ‡§æ / Good' if size_gb < 5 else '‚ö†Ô∏è ‡§¨‡§°‡§º‡§æ ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü / Large dataset'}
    """


# Function 13: Clean Invalid Entries
def clean_invalid_entries(progress=gr.Progress()):
    """Remove entries with invalid/corrupted audio"""
    output_dir = "tts_training_data"
    metadata_file = os.path.join(output_dir, "metadata.csv")
    
    if not os.path.exists(metadata_file):
        return "‚ùå ‡§ï‡•ã‡§à ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ! / No dataset found!"
    
    progress(0, desc="‡§Ö‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§è‡§Ç‡§ü‡•ç‡§∞‡•Ä‡§ú‡§º ‡§∏‡§æ‡§´‡§º ‡§π‡•ã ‡§∞‡§π‡•Ä ‡§π‡•à‡§Ç... / Cleaning invalid entries...")
    
    with open(metadata_file, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    valid_lines = []
    invalid_count = 0
    
    for i, line in enumerate(lines):
        if "|" in line:
            filename, text = line.strip().split("|", 1)
            audio_path = os.path.join(output_dir, "filtered_chunks", filename)
            
            try:
                # Try to load audio file
                audio = AudioSegment.from_file(audio_path)
                if len(audio) > 500:  # At least 0.5 seconds
                    valid_lines.append(line)
                else:
                    invalid_count += 1
            except:
                invalid_count += 1
        
        progress((i + 1) / len(lines))
    
    cleaned_file = os.path.join(output_dir, "metadata_cleaned.csv")
    with open(cleaned_file, "w", encoding="utf-8") as f:
        f.writelines(valid_lines)
    
    progress(1.0)
    
    return f"""
‚úÖ ‡§∏‡§´‡§æ‡§à ‡§™‡•Ç‡§∞‡•ç‡§£ / Cleaning Complete!

üìä ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results:
- ‡§Æ‡•Ç‡§≤ ‡§è‡§Ç‡§ü‡•ç‡§∞‡•Ä‡§ú‡§º / Original entries: {len(lines)}
- ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§è‡§Ç‡§ü‡•ç‡§∞‡•Ä‡§ú‡§º / Valid entries: {len(valid_lines)}
- ‡§π‡§ü‡§æ‡§à ‡§ó‡§à / Removed: {invalid_count}

üìÅ ‡§∏‡§æ‡§´‡§º ‡§ï‡•Ä ‡§ó‡§à ‡§´‡§º‡§æ‡§á‡§≤ / Cleaned file: {cleaned_file}
    """


# Function 14: Create Backup
def create_backup(progress=gr.Progress()):
    """Create backup of entire dataset"""
    output_dir = "tts_training_data"
    
    if not os.path.exists(output_dir):
        return "‚ùå ‡§ï‡•ã‡§à ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ! / No dataset found!"
    
    progress(0, desc="‡§¨‡•à‡§ï‡§Ö‡§™ ‡§¨‡§®‡§æ‡§Ø‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à... / Creating backup...")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_dir = f"tts_training_data_backup_{timestamp}"
    
    shutil.copytree(output_dir, backup_dir)
    
    progress(1.0)
    
    return f"""
‚úÖ ‡§¨‡•à‡§ï‡§Ö‡§™ ‡§™‡•Ç‡§∞‡•ç‡§£ / Backup Complete!

üìÅ ‡§¨‡•à‡§ï‡§Ö‡§™ ‡§∏‡•ç‡§•‡§æ‡§® / Backup location: {backup_dir}
‚è∞ ‡§ü‡§æ‡§á‡§Æ‡§∏‡•ç‡§ü‡•à‡§Æ‡•ç‡§™ / Timestamp: {timestamp}

üí° ‡§®‡•ã‡§ü / Note: ‡§Æ‡•Ç‡§≤ ‡§´‡§º‡•ã‡§≤‡•ç‡§°‡§∞ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§π‡•à / Original folder is safe
    """


# Function 15: Normalize All Audio
def normalize_all_audio(progress=gr.Progress()):
    """Normalize volume of all audio files"""
    output_dir = "tts_training_data"
    chunks_dir = os.path.join(output_dir, "filtered_chunks")
    
    if not os.path.exists(chunks_dir):
        return "‚ùå ‡§ï‡•ã‡§à ‡§ë‡§°‡§ø‡§Ø‡•ã ‡§ö‡§Ç‡§ï‡•ç‡§∏ ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•á! / No audio chunks found!"
    
    progress(0, desc="‡§∏‡§≠‡•Ä ‡§ë‡§°‡§ø‡§Ø‡•ã ‡§®‡•â‡§∞‡•ç‡§Æ‡§≤‡§æ‡§á‡§ú‡§º ‡§π‡•ã ‡§∞‡§π‡•á ‡§π‡•à‡§Ç... / Normalizing all audio...")
    
    audio_files = [f for f in os.listdir(chunks_dir) if f.endswith('.wav')]
    normalized_count = 0
    
    for i, filename in enumerate(audio_files):
        file_path = os.path.join(chunks_dir, filename)
        try:
            audio = AudioSegment.from_file(file_path)
            normalized_audio = normalize(audio)
            normalized_audio.export(file_path, format="wav")
            normalized_count += 1
        except:
            pass
        
        progress((i + 1) / len(audio_files))
    
    return f"""
‚úÖ ‡§®‡•â‡§∞‡•ç‡§Æ‡§≤‡§æ‡§á‡§ú‡§º‡•á‡§∂‡§® ‡§™‡•Ç‡§∞‡•ç‡§£ / Normalization Complete!

üìä ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results:
- ‡§ï‡•Å‡§≤ ‡§´‡§º‡§æ‡§á‡§≤‡•á‡§Ç / Total files: {len(audio_files)}
- ‡§®‡•â‡§∞‡•ç‡§Æ‡§≤‡§æ‡§á‡§ú‡§º ‡§ï‡•Ä ‡§ó‡§à‡§Ç / Normalized: {normalized_count}
- ‡§µ‡§ø‡§´‡§≤ / Failed: {len(audio_files) - normalized_count}

üîä ‡§∏‡§≠‡•Ä ‡§ë‡§°‡§ø‡§Ø‡•ã ‡§´‡§º‡§æ‡§á‡§≤‡•á‡§Ç ‡§Ö‡§¨ ‡§∏‡§Æ‡§æ‡§® ‡§µ‡•â‡§≤‡•ç‡§Ø‡•Ç‡§Æ ‡§™‡§∞ ‡§π‡•à‡§Ç! / All audio files now have consistent volume!
    """


# Function 16: Generate Statistics Chart
def generate_statistics_chart(progress=gr.Progress()):
    """Generate visual statistics chart"""
    output_dir = "tts_training_data"
    stats_file = os.path.join(output_dir, "statistics", "audio_statistics.json")
    
    if not os.path.exists(stats_file):
        return "‚ùå ‡§ï‡•ã‡§à ‡§∏‡§æ‡§Ç‡§ñ‡•ç‡§Ø‡§ø‡§ï‡•Ä ‡§´‡§º‡§æ‡§á‡§≤ ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•Ä! / No statistics file found!"
    
    progress(0, desc="‡§ö‡§æ‡§∞‡•ç‡§ü ‡§¨‡§®‡§æ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç... / Generating chart...")
    
    with open(stats_file, "r", encoding="utf-8") as f:
        stats = json.load(f)
    
    durations = [s['duration'] for s in stats]
    word_counts = [s['word_count'] for s in stats]
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    ax1.hist(durations, bins=20, color='skyblue', edgecolor='black')
    ax1.set_title('Duration Distribution / ‡§Ö‡§µ‡§ß‡§ø ‡§µ‡§ø‡§§‡§∞‡§£')
    ax1.set_xlabel('Seconds / ‡§∏‡•á‡§ï‡§Ç‡§°')
    ax1.set_ylabel('Count / ‡§ó‡§£‡§®‡§æ')
    
    ax2.hist(word_counts, bins=20, color='lightgreen', edgecolor='black')
    ax2.set_title('Word Count Distribution / ‡§∂‡§¨‡•ç‡§¶ ‡§ó‡§£‡§®‡§æ ‡§µ‡§ø‡§§‡§∞‡§£')
    ax2.set_xlabel('Words / ‡§∂‡§¨‡•ç‡§¶')
    ax2.set_ylabel('Count / ‡§ó‡§£‡§®‡§æ')
    
    plt.tight_layout()
    
    chart_file = os.path.join(output_dir, "statistics", "statistics_chart.png")
    plt.savefig(chart_file)
    plt.close()
    
    progress(1.0)
    
    return f"""
‚úÖ ‡§ö‡§æ‡§∞‡•ç‡§ü ‡§¨‡§®‡§æ‡§Ø‡§æ ‡§ó‡§Ø‡§æ / Chart Generated!

üìä ‡§ö‡§æ‡§∞‡•ç‡§ü ‡§´‡§º‡§æ‡§á‡§≤ / Chart file: {chart_file}

üí° ‡§ö‡§æ‡§∞‡•ç‡§ü ‡§Æ‡•á‡§Ç ‡§¶‡§ø‡§ñ‡§æ‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à / Chart shows:
- ‡§Ö‡§µ‡§ß‡§ø ‡§µ‡§ø‡§§‡§∞‡§£ / Duration distribution
- ‡§∂‡§¨‡•ç‡§¶ ‡§ó‡§£‡§®‡§æ ‡§µ‡§ø‡§§‡§∞‡§£ / Word count distribution
    """


# Function 17: Find Long Samples
def find_long_samples(max_duration, progress=gr.Progress()):
    """Find samples longer than specified duration"""
    output_dir = "tts_training_data"
    stats_file = os.path.join(output_dir, "statistics", "audio_statistics.json")
    
    if not os.path.exists(stats_file):
        return "‚ùå ‡§ï‡•ã‡§à ‡§∏‡§æ‡§Ç‡§ñ‡•ç‡§Ø‡§ø‡§ï‡•Ä ‡§´‡§º‡§æ‡§á‡§≤ ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•Ä! / No statistics file found!"
    
    progress(0, desc="‡§≤‡§Ç‡§¨‡•á ‡§®‡§Æ‡•Ç‡§®‡•á ‡§ñ‡•ã‡§ú‡•á ‡§ú‡§æ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç... / Finding long samples...")
    
    with open(stats_file, "r", encoding="utf-8") as f:
        stats = json.load(f)
    
    long_samples = [s for s in stats if s['duration'] > max_duration]
    
    progress(1.0)
    
    if long_samples:
        preview = "\n".join([f"- {s['filename']}: {s['duration']:.2f}s" for s in long_samples[:10]])
        return f"""
‚úÖ ‡§ñ‡•ã‡§ú ‡§™‡•Ç‡§∞‡•ç‡§£ / Search Complete!

üìä ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results:
- {max_duration}s ‡§∏‡•á ‡§≤‡§Ç‡§¨‡•á ‡§®‡§Æ‡•Ç‡§®‡•á / Samples longer than {max_duration}s: {len(long_samples)}

üìù ‡§™‡§π‡§≤‡•á 10 ‡§®‡§Æ‡•Ç‡§®‡•á / First 10 Samples:
{preview}
{'...(‡§î‡§∞ ‡§Ö‡§ß‡§ø‡§ï / and more)' if len(long_samples) > 10 else ''}
        """
    else:
        return f"‚úÖ ‡§ï‡•ã‡§à ‡§®‡§Æ‡•Ç‡§®‡§æ {max_duration}s ‡§∏‡•á ‡§≤‡§Ç‡§¨‡§æ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à! / No samples longer than {max_duration}s!"


# Function 18: Text Length Filter
def text_length_filter(min_chars, max_chars, progress=gr.Progress()):
    """Filter samples by text length"""
    output_dir = "tts_training_data"
    metadata_file = os.path.join(output_dir, "metadata.csv")
    
    if not os.path.exists(metadata_file):
        return "‚ùå ‡§ï‡•ã‡§à ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ! / No dataset found!"
    
    progress(0, desc="‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§≤‡§Ç‡§¨‡§æ‡§à ‡§∏‡•á ‡§´‡§º‡§ø‡§≤‡•ç‡§ü‡§∞ ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•à... / Filtering by text length...")
    
    with open(metadata_file, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    filtered_lines = []
    for line in lines:
        if "|" in line:
            filename, text = line.strip().split("|", 1)
            text_len = len(text)
            if min_chars <= text_len <= max_chars:
                filtered_lines.append(line)
    
    filtered_file = os.path.join(output_dir, "metadata_length_filtered.csv")
    with open(filtered_file, "w", encoding="utf-8") as f:
        f.writelines(filtered_lines)
    
    progress(1.0)
    
    return f"""
‚úÖ ‡§´‡§º‡§ø‡§≤‡•ç‡§ü‡§∞‡§ø‡§Ç‡§ó ‡§™‡•Ç‡§∞‡•ç‡§£ / Filtering Complete!

üìä ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results:
- ‡§Æ‡•Ç‡§≤ ‡§®‡§Æ‡•Ç‡§®‡•á / Original samples: {len(lines)}
- ‡§´‡§º‡§ø‡§≤‡•ç‡§ü‡§∞ ‡§ï‡§ø‡§è ‡§ó‡§è ‡§®‡§Æ‡•Ç‡§®‡•á / Filtered samples: {len(filtered_lines)}
- ‡§π‡§ü‡§æ‡§è ‡§ó‡§è / Removed: {len(lines) - len(filtered_lines)}

üìè ‡§≤‡§Ç‡§¨‡§æ‡§à ‡§∏‡•Ä‡§Æ‡§æ / Length range: {min_chars}-{max_chars} characters

üìÅ ‡§´‡§º‡§æ‡§á‡§≤ ‡§∏‡§π‡•á‡§ú‡•Ä ‡§ó‡§à / File saved: {filtered_file}
    """


# Function 19: Create Dataset Splits
def create_dataset_splits(train_pct, val_pct, test_pct, progress=gr.Progress()):
    """Create train/val/test splits"""
    output_dir = "tts_training_data"
    metadata_file = os.path.join(output_dir, "metadata.csv")
    
    if not os.path.exists(metadata_file):
        return "‚ùå ‡§ï‡•ã‡§à ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ! / No dataset found!", None, None, None
    
    if train_pct + val_pct + test_pct != 100:
        return "‚ùå ‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§ ‡§ï‡§æ ‡§Ø‡•ã‡§ó 100 ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è! / Percentages must sum to 100!", None, None, None
    
    progress(0, desc="‡§µ‡§ø‡§≠‡§æ‡§ú‡§® ‡§¨‡§®‡§æ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç... / Creating splits...")
    
    with open(metadata_file, "r", encoding="utf-8") as f:
        lines = [l for l in f.readlines() if "|" in l]
    
    np.random.shuffle(lines)
    
    train_idx = int(len(lines) * train_pct / 100)
    val_idx = train_idx + int(len(lines) * val_pct / 100)
    
    train_lines = lines[:train_idx]
    val_lines = lines[train_idx:val_idx]
    test_lines = lines[val_idx:]
    
    train_file = os.path.join(output_dir, "train.txt")
    val_file = os.path.join(output_dir, "val.txt")
    test_file = os.path.join(output_dir, "test.txt")
    
    with open(train_file, "w", encoding="utf-8") as f:
        f.writelines(train_lines)
    with open(val_file, "w", encoding="utf-8") as f:
        f.writelines(val_lines)
    with open(test_file, "w", encoding="utf-8") as f:
        f.writelines(test_lines)
    
    progress(1.0)
    
    summary = f"""
‚úÖ ‡§µ‡§ø‡§≠‡§æ‡§ú‡§® ‡§™‡•Ç‡§∞‡•ç‡§£ / Splits Complete!

üìä ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results:
- ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ / Training: {len(train_lines)} ({train_pct}%)
- ‡§∏‡§§‡•ç‡§Ø‡§æ‡§™‡§® / Validation: {len(val_lines)} ({val_pct}%)
- ‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£ / Test: {len(test_lines)} ({test_pct}%)

üìÅ ‡§´‡§º‡§æ‡§á‡§≤‡•á‡§Ç / Files:
- {train_file}
- {val_file}
- {test_file}
    """
    
    return summary, train_file, val_file, test_file


# Function 20: Export Dataset Summary
def export_dataset_summary(progress=gr.Progress()):
    """Export comprehensive dataset summary"""
    output_dir = "tts_training_data"
    metadata_file = os.path.join(output_dir, "metadata.csv")
    stats_file = os.path.join(output_dir, "statistics", "audio_statistics.json")
    
    if not os.path.exists(metadata_file):
        return "‚ùå ‡§ï‡•ã‡§à ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ! / No dataset found!"
    
    progress(0, desc="‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ ‡§è‡§ï‡•ç‡§∏‡§™‡•ã‡§∞‡•ç‡§ü ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•à... / Exporting summary...")
    
    with open(metadata_file, "r", encoding="utf-8") as f:
        lines = [l for l in f.readlines() if "|" in l]
    
    summary_data = {
        "total_samples": len(lines),
        "total_duration": 0,
        "total_words": 0,
        "total_characters": 0,
        "avg_duration": 0,
        "avg_words": 0,
        "avg_characters": 0,
        "generated_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    
    for line in lines:
        if "|" in line:
            _, text = line.strip().split("|", 1)
            summary_data["total_words"] += len(text.split())
            summary_data["total_characters"] += len(text)
    
    if os.path.exists(stats_file):
        with open(stats_file, "r", encoding="utf-8") as f:
            stats = json.load(f)
            summary_data["total_duration"] = sum(s['duration'] for s in stats)
    
    if summary_data["total_samples"] > 0:
        summary_data["avg_duration"] = summary_data["total_duration"] / summary_data["total_samples"]
        summary_data["avg_words"] = summary_data["total_words"] / summary_data["total_samples"]
        summary_data["avg_characters"] = summary_data["total_characters"] / summary_data["total_samples"]
    
    summary_file = os.path.join(output_dir, "dataset_summary.json")
    with open(summary_file, "w", encoding="utf-8") as f:
        json.dump(summary_data, f, indent=2)
    
    progress(1.0)
    
    return f"""
‚úÖ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ ‡§è‡§ï‡•ç‡§∏‡§™‡•ã‡§∞‡•ç‡§ü ‡§™‡•Ç‡§∞‡•ç‡§£ / Summary Export Complete!

üìä ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ / Dataset Summary:
- ‡§ï‡•Å‡§≤ ‡§®‡§Æ‡•Ç‡§®‡•á / Total samples: {summary_data['total_samples']}
- ‡§ï‡•Å‡§≤ ‡§Ö‡§µ‡§ß‡§ø / Total duration: {summary_data['total_duration']:.2f}s
- ‡§ï‡•Å‡§≤ ‡§∂‡§¨‡•ç‡§¶ / Total words: {summary_data['total_words']}
- ‡§î‡§∏‡§§ ‡§Ö‡§µ‡§ß‡§ø / Avg duration: {summary_data['avg_duration']:.2f}s
- ‡§î‡§∏‡§§ ‡§∂‡§¨‡•ç‡§¶ / Avg words: {summary_data['avg_words']:.1f}
- ‡§î‡§∏‡§§ ‡§µ‡§∞‡•ç‡§£ / Avg characters: {summary_data['avg_characters']:.1f}

üìÅ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ ‡§´‡§º‡§æ‡§á‡§≤ / Summary file: {summary_file}
    """


def clear_output():
    """Clear output directory"""
    output_dir = "tts_training_data"
    if os.path.exists(output_dir):
        shutil.rmtree(output_dir)
    return "‚úÖ ‡§Ü‡§â‡§ü‡§™‡•Å‡§ü ‡§´‡§º‡•ã‡§≤‡•ç‡§°‡§∞ ‡§∏‡§æ‡§´‡§º ‡§π‡•ã ‡§ó‡§Ø‡§æ! / Output folder cleared!", None, None, None


# Create Gradio interface
with gr.Blocks(title="TTS ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§°‡•á‡§ü‡§æ ‡§§‡•à‡§Ø‡§æ‡§∞‡•Ä ‡§â‡§™‡§ï‡§∞‡§£ / TTS Training Data Preparation Tool", theme=gr.themes.Soft()) as demo:
    
    gr.Markdown("""
    # üéôÔ∏è TTS ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§°‡•á‡§ü‡§æ ‡§§‡•à‡§Ø‡§æ‡§∞‡•Ä ‡§â‡§™‡§ï‡§∞‡§£ / TTS Training Data Preparation Tool
    
    ‡§ï‡§ø‡§∏‡•Ä ‡§≠‡•Ä ‡§≤‡§Ç‡§¨‡§æ‡§à ‡§ï‡•Ä ‡§ë‡§°‡§ø‡§Ø‡•ã ‡§´‡§º‡§æ‡§á‡§≤ ‡§Ö‡§™‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç! / Upload an audio file of any length!
    
    **‡§∏‡§Æ‡§∞‡•ç‡§•‡§ø‡§§ ‡§™‡•ç‡§∞‡§æ‡§∞‡•Ç‡§™ / Supported formats:** MP3, WAV, FLAC, OGG, M4A, ‡§î‡§∞ ‡§Ö‡§ß‡§ø‡§ï / and more!
    """)
    
    # YouTube Channel Promotion
    gr.Markdown("""
    <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px; text-align: center; margin: 20px 0;">
        <h2 style="color: white; margin: 0;">üé¨ Mind Hack Secrets</h2>
        <p style="color: white; margin: 10px 0;">AI, TTS, ‡§î‡§∞ Technology Tutorials ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Æ‡•á‡§∞‡•á YouTube ‡§ö‡•à‡§®‡§≤ ‡§ï‡•ã ‡§∏‡§¨‡•ç‡§∏‡§ï‡•ç‡§∞‡§æ‡§á‡§¨ ‡§ï‡§∞‡•á‡§Ç!</p>
        <p style="color: white; margin: 10px 0;">Subscribe to my YouTube channel for AI, TTS, and Technology Tutorials!</p>
        <a href="https://www.youtube.com/@MindHackSecrets-o4y?sub_confirmation=1" target="_blank">
            <button style="background-color: #FF0000; color: white; padding: 15px 30px; font-size: 18px; border: none; border-radius: 5px; cursor: pointer; font-weight: bold;">
                üîî Subscribe Now / ‡§Ö‡§≠‡•Ä ‡§∏‡§¨‡•ç‡§∏‡§ï‡•ç‡§∞‡§æ‡§á‡§¨ ‡§ï‡§∞‡•á‡§Ç
            </button>
        </a>
    </div>
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### ‚öôÔ∏è ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§∏‡•á‡§ü‡§ø‡§Ç‡§ó‡•ç‡§∏ / Main Settings")
            audio_input = gr.Audio(
                label="‡§ë‡§°‡§ø‡§Ø‡•ã ‡§´‡§º‡§æ‡§á‡§≤ ‡§Ö‡§™‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç / Upload Audio File",
                type="filepath",
                sources=["upload"]
            )
            
            language = gr.Radio(
                choices=["Hindi (‡§π‡§ø‡§Ç‡§¶‡•Ä)", "English (‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä)"],
                value="Hindi (‡§π‡§ø‡§Ç‡§¶‡•Ä)",
                label="‡§≠‡§æ‡§∑‡§æ / Language"
            )
            
            chunk_duration = gr.Slider(
                minimum=5,
                maximum=30,
                value=10,
                step=1,
                label="‡§ö‡§Ç‡§ï ‡§Ö‡§µ‡§ß‡§ø (‡§∏‡•á‡§ï‡§Ç‡§°) / Chunk Duration (seconds)"
            )
            
            gr.Markdown("### üîß ‡§â‡§®‡•ç‡§®‡§§ ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™ / Advanced Options")
            
            normalize_audio = gr.Checkbox(
                label="‡§ë‡§°‡§ø‡§Ø‡•ã ‡§®‡•â‡§∞‡•ç‡§Æ‡§≤‡§æ‡§á‡§ú‡§º ‡§ï‡§∞‡•á‡§Ç / Normalize Audio",
                value=True
            )
            
            remove_silence = gr.Checkbox(
                label="‡§Æ‡•å‡§® ‡§π‡§ü‡§æ‡§è‡§Å / Remove Silence",
                value=True
            )
            
            apply_noise_reduction = gr.Checkbox(
                label="‡§∂‡•ã‡§∞ ‡§Æ‡•á‡§Ç ‡§ï‡§Æ‡•Ä / Noise Reduction",
                value=False
            )
            
            with gr.Row():
                min_duration = gr.Slider(
                    minimum=1,
                    maximum=10,
                    value=3,
                    step=0.5,
                    label="‡§®‡•ç‡§Ø‡•Ç‡§®‡§§‡§Æ ‡§Ö‡§µ‡§ß‡§ø / Min Duration (s)"
                )
                
                max_duration = gr.Slider(
                    minimum=10,
                    maximum=60,
                    value=30,
                    step=1,
                    label="‡§Ö‡§ß‡§ø‡§ï‡§§‡§Æ ‡§Ö‡§µ‡§ß‡§ø / Max Duration (s)"
                )
            
            with gr.Row():
                process_btn = gr.Button("üöÄ ‡§â‡§§‡•ç‡§™‡§®‡•ç‡§® ‡§ï‡§∞‡•á‡§Ç / Generate & Process", variant="primary", size="lg")
                clear_btn = gr.Button("üóëÔ∏è ‡§∏‡§æ‡§´‡§º ‡§ï‡§∞‡•á‡§Ç / Clear Output", variant="secondary")
        
        with gr.Column(scale=2):
            output_text = gr.Textbox(
                label="‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏‡§ø‡§Ç‡§ó ‡§∏‡•ç‡§•‡§ø‡§§‡§ø / Processing Status",
                lines=20,
                max_lines=25
            )
            
            with gr.Row():
                transcript_file = gr.File(label="üìÑ ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§∂‡§® / Transcriptions")
                metadata_file = gr.File(label="üìã ‡§Æ‡•á‡§ü‡§æ‡§°‡•á‡§ü‡§æ / Metadata")
            
            stats_file = gr.File(label="üìä ‡§∏‡§æ‡§Ç‡§ñ‡•ç‡§Ø‡§ø‡§ï‡•Ä / Statistics")
    
    gr.Markdown("---")
    gr.Markdown("## üõ†Ô∏è 20 ‡§∂‡§ï‡•ç‡§§‡§ø‡§∂‡§æ‡§≤‡•Ä AI ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§ï‡§æ‡§∞‡•ç‡§Ø / 20 Powerful AI Training Functions")
    
    with gr.Tabs():
        with gr.Tab("1Ô∏è‚É£ ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§∏‡§§‡•ç‡§Ø‡§æ‡§™‡§® / Dataset Validation"):
            validate_btn = gr.Button("‚úÖ ‡§∏‡§§‡•ç‡§Ø‡§æ‡§™‡§ø‡§§ ‡§ï‡§∞‡•á‡§Ç / Validate", variant="primary")
            validation_output = gr.Textbox(label="‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results", lines=12)
        
        with gr.Tab("2Ô∏è‚É£ ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§∏‡§Ç‡§§‡•Å‡§≤‡§® / Balance Dataset"):
            min_samples = gr.Slider(1, 20, 5, step=1, label="‡§®‡•ç‡§Ø‡•Ç‡§®‡§§‡§Æ ‡§®‡§Æ‡•Ç‡§®‡•á / Min samples")
            balance_btn = gr.Button("‚öñÔ∏è ‡§∏‡§Ç‡§§‡•Å‡§≤‡§ø‡§§ ‡§ï‡§∞‡•á‡§Ç / Balance", variant="primary")
            balance_output = gr.Textbox(label="‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results", lines=10)
        
        with gr.Tab("3Ô∏è‚É£ ‡§ü‡•ç‡§∞‡•á‡§®/‡§µ‡•à‡§≤ ‡§µ‡§ø‡§≠‡§æ‡§ú‡§® / Train/Val Split"):
            train_split = gr.Slider(60, 95, 80, step=5, label="‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ % / Training %")
            export_btn = gr.Button("üì§ ‡§è‡§ï‡•ç‡§∏‡§™‡•ã‡§∞‡•ç‡§ü / Export", variant="primary")
            export_output = gr.Textbox(label="‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results", lines=10)
            with gr.Row():
                train_file_out = gr.File(label="üéì ‡§ü‡•ç‡§∞‡•á‡§® / Train")
                val_file_out = gr.File(label="‚úÖ ‡§µ‡•à‡§≤ / Val")
        
        with gr.Tab("4Ô∏è‚É£ ‡§°‡•Å‡§™‡•ç‡§≤‡•Ä‡§ï‡•á‡§ü ‡§ï‡§æ ‡§™‡§§‡§æ ‡§≤‡§ó‡§æ‡§è‡§Ç / Detect Duplicates"):
            detect_dup_btn = gr.Button("üîç ‡§°‡•Å‡§™‡•ç‡§≤‡•Ä‡§ï‡•á‡§ü ‡§ñ‡•ã‡§ú‡•á‡§Ç / Find Duplicates", variant="primary")
            dup_output = gr.Textbox(label="‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results", lines=12)
        
        with gr.Tab("5Ô∏è‚É£ ‡§µ‡§∞‡•ç‡§£ ‡§ó‡§£‡§®‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ / Character Count"):
            char_btn = gr.Button("üìä ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§ï‡§∞‡•á‡§Ç / Analyze", variant="primary")
            char_output = gr.Textbox(label="‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results", lines=10)
        
        with gr.Tab("6Ô∏è‚É£ ‡§∂‡§¨‡•ç‡§¶ ‡§Ü‡§µ‡•É‡§§‡•ç‡§§‡§ø / Word Frequency"):
            top_n = gr.Slider(5, 50, 20, step=5, label="‡§∂‡•Ä‡§∞‡•ç‡§∑ ‡§∂‡§¨‡•ç‡§¶ / Top words")
            word_freq_btn = gr.Button("üìà ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§ï‡§∞‡•á‡§Ç / Analyze", variant="primary")
            word_freq_output = gr.Textbox(label="‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results", lines=12)
        
        with gr.Tab("7Ô∏è‚É£ ‡§õ‡•ã‡§ü‡•á ‡§®‡§Æ‡•Ç‡§®‡•á ‡§π‡§ü‡§æ‡§è‡§Ç / Remove Short Samples"):
            min_words = gr.Slider(3, 20, 5, step=1, label="‡§®‡•ç‡§Ø‡•Ç‡§®‡§§‡§Æ ‡§∂‡§¨‡•ç‡§¶ / Min words")
            remove_short_btn = gr.Button("üóëÔ∏è ‡§π‡§ü‡§æ‡§è‡§Ç / Remove", variant="primary")
            remove_short_output = gr.Textbox(label="‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results", lines=8)
        
        with gr.Tab("8Ô∏è‚É£ ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü / Dataset Report"):
            report_btn = gr.Button("üìã ‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü ‡§¨‡§®‡§æ‡§è‡§Ç / Generate Report", variant="primary")
            report_output = gr.Textbox(label="‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results", lines=15)
        
        with gr.Tab("9Ô∏è‚É£ ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§Æ‡§∞‡•ç‡§ú ‡§ï‡§∞‡•á‡§Ç / Merge Datasets"):
            merge_btn = gr.Button("üîó ‡§Æ‡§∞‡•ç‡§ú ‡§ï‡§∞‡•á‡§Ç / Merge", variant="primary")
            merge_output = gr.Textbox(label="‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results", lines=10)
        
        with gr.Tab("üîü JSON ‡§Æ‡•á‡§Ç ‡§è‡§ï‡•ç‡§∏‡§™‡•ã‡§∞‡•ç‡§ü / Export to JSON"):
            json_btn = gr.Button("üì¶ JSON ‡§è‡§ï‡•ç‡§∏‡§™‡•ã‡§∞‡•ç‡§ü / Export JSON", variant="primary")
            json_output = gr.Textbox(label="‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results", lines=8)
        
        with gr.Tab("1Ô∏è‚É£1Ô∏è‚É£ ‡§∞‡•à‡§Ç‡§°‡§Æ ‡§∏‡•à‡§Ç‡§™‡§≤ / Random Sample"):
            num_samples = gr.Slider(5, 100, 20, step=5, label="‡§®‡§Æ‡•Ç‡§®‡•á / Samples")
            sample_btn = gr.Button("üé≤ ‡§∏‡•à‡§Ç‡§™‡§≤ ‡§ï‡§∞‡•á‡§Ç / Sample", variant="primary")
            sample_output = gr.Textbox(label="‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results", lines=12)
        
        with gr.Tab("1Ô∏è‚É£2Ô∏è‚É£ ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§∏‡§æ‡§á‡§ú‡§º / Dataset Size"):
            size_btn = gr.Button("üíæ ‡§∏‡§æ‡§á‡§ú‡§º ‡§ó‡§£‡§®‡§æ / Calculate Size", variant="primary")
            size_output = gr.Textbox(label="‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results", lines=10)
        
        with gr.Tab("1Ô∏è‚É£3Ô∏è‚É£ ‡§Ö‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§∏‡§æ‡§´‡§º ‡§ï‡§∞‡•á‡§Ç / Clean Invalid"):
            clean_btn = gr.Button("üßπ ‡§∏‡§æ‡§´‡§º ‡§ï‡§∞‡•á‡§Ç / Clean", variant="primary")
            clean_output = gr.Textbox(label="‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results", lines=10)
        
        with gr.Tab("1Ô∏è‚É£4Ô∏è‚É£ ‡§¨‡•à‡§ï‡§Ö‡§™ ‡§¨‡§®‡§æ‡§è‡§Ç / Create Backup"):
            backup_btn = gr.Button("üíæ ‡§¨‡•à‡§ï‡§Ö‡§™ / Backup", variant="primary")
            backup_output = gr.Textbox(label="‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results", lines=8)
        
        with gr.Tab("1Ô∏è‚É£5Ô∏è‚É£ ‡§∏‡§≠‡•Ä ‡§®‡•â‡§∞‡•ç‡§Æ‡§≤‡§æ‡§á‡§ú‡§º ‡§ï‡§∞‡•á‡§Ç / Normalize All"):
            normalize_btn = gr.Button("üîä ‡§®‡•â‡§∞‡•ç‡§Æ‡§≤‡§æ‡§á‡§ú‡§º / Normalize", variant="primary")
            normalize_output = gr.Textbox(label="‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results", lines=10)
        
        with gr.Tab("1Ô∏è‚É£6Ô∏è‚É£ ‡§ö‡§æ‡§∞‡•ç‡§ü ‡§¨‡§®‡§æ‡§è‡§Ç / Generate Chart"):
            chart_btn = gr.Button("üìä ‡§ö‡§æ‡§∞‡•ç‡§ü / Chart", variant="primary")
            chart_output = gr.Textbox(label="‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results", lines=8)
        
        with gr.Tab("1Ô∏è‚É£7Ô∏è‚É£ ‡§≤‡§Ç‡§¨‡•á ‡§®‡§Æ‡•Ç‡§®‡•á ‡§ñ‡•ã‡§ú‡•á‡§Ç / Find Long Samples"):
            max_dur = gr.Slider(10, 60, 20, step=5, label="‡§Ö‡§ß‡§ø‡§ï‡§§‡§Æ ‡§Ö‡§µ‡§ß‡§ø / Max duration (s)")
            long_btn = gr.Button("üîé ‡§ñ‡•ã‡§ú‡•á‡§Ç / Find", variant="primary")
            long_output = gr.Textbox(label="‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results", lines=12)
        
        with gr.Tab("1Ô∏è‚É£8Ô∏è‚É£ ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§≤‡§Ç‡§¨‡§æ‡§à ‡§´‡§º‡§ø‡§≤‡•ç‡§ü‡§∞ / Text Length Filter"):
            with gr.Row():
                min_chars = gr.Slider(10, 200, 50, step=10, label="‡§®‡•ç‡§Ø‡•Ç‡§®‡§§‡§Æ ‡§µ‡§∞‡•ç‡§£ / Min chars")
                max_chars = gr.Slider(200, 1000, 500, step=50, label="‡§Ö‡§ß‡§ø‡§ï‡§§‡§Æ ‡§µ‡§∞‡•ç‡§£ / Max chars")
            text_filter_btn = gr.Button("üîç ‡§´‡§º‡§ø‡§≤‡•ç‡§ü‡§∞ / Filter", variant="primary")
            text_filter_output = gr.Textbox(label="‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results", lines=10)
        
        with gr.Tab("1Ô∏è‚É£9Ô∏è‚É£ 3-Way ‡§∏‡•ç‡§™‡•ç‡§≤‡§ø‡§ü / 3-Way Split"):
            with gr.Row():
                train_pct = gr.Slider(50, 90, 70, step=5, label="‡§ü‡•ç‡§∞‡•á‡§® % / Train %")
                val_pct = gr.Slider(5, 30, 15, step=5, label="‡§µ‡•à‡§≤ % / Val %")
                test_pct = gr.Slider(5, 30, 15, step=5, label="‡§ü‡•á‡§∏‡•ç‡§ü % / Test %")
            split3_btn = gr.Button("üìä ‡§µ‡§ø‡§≠‡§æ‡§ú‡§® / Split", variant="primary")
            split3_output = gr.Textbox(label="‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results", lines=10)
            with gr.Row():
                train3_file = gr.File(label="‡§ü‡•ç‡§∞‡•á‡§® / Train")
                val3_file = gr.File(label="‡§µ‡•à‡§≤ / Val")
                test3_file = gr.File(label="‡§ü‡•á‡§∏‡•ç‡§ü / Test")
        
        with gr.Tab("2Ô∏è‚É£0Ô∏è‚É£ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ ‡§è‡§ï‡•ç‡§∏‡§™‡•ã‡§∞‡•ç‡§ü / Export Summary"):
            summary_btn = gr.Button("üìÑ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ / Summary", variant="primary")
            summary_output = gr.Textbox(label="‡§™‡§∞‡§ø‡§£‡§æ‡§Æ / Results", lines=15)
    
    # Event handlers
    process_btn.click(
        fn=process_audio,
        inputs=[audio_input, language, chunk_duration, normalize_audio, remove_silence, 
                apply_noise_reduction, min_duration, max_duration],
        outputs=[output_text, transcript_file, metadata_file, stats_file]
    )
    
    clear_btn.click(
        fn=clear_output,
        inputs=[],
        outputs=[output_text, transcript_file, metadata_file, stats_file]
    )
    
    validate_btn.click(fn=validate_dataset, outputs=[validation_output])
    balance_btn.click(fn=balance_dataset, inputs=[min_samples], outputs=[balance_output])
    export_btn.click(fn=export_for_training, inputs=[train_split], outputs=[export_output, train_file_out, val_file_out])
    detect_dup_btn.click(fn=detect_duplicates, outputs=[dup_output])
    char_btn.click(fn=character_count_analysis, outputs=[char_output])
    word_freq_btn.click(fn=word_frequency_analysis, inputs=[top_n], outputs=[word_freq_output])
    remove_short_btn.click(fn=remove_short_samples, inputs=[min_words], outputs=[remove_short_output])
    report_btn.click(fn=generate_dataset_report, outputs=[report_output])
    merge_btn.click(fn=merge_datasets, outputs=[merge_output])
    json_btn.click(fn=export_to_json, outputs=[json_output])
    sample_btn.click(fn=sample_random_entries, inputs=[num_samples], outputs=[sample_output])
    size_btn.click(fn=calculate_dataset_size, outputs=[size_output])
    clean_btn.click(fn=clean_invalid_entries, outputs=[clean_output])
    backup_btn.click(fn=create_backup, outputs=[backup_output])
    normalize_btn.click(fn=normalize_all_audio, outputs=[normalize_output])
    chart_btn.click(fn=generate_statistics_chart, outputs=[chart_output])
    long_btn.click(fn=find_long_samples, inputs=[max_dur], outputs=[long_output])
    text_filter_btn.click(fn=text_length_filter, inputs=[min_chars, max_chars], outputs=[text_filter_output])
    split3_btn.click(fn=create_dataset_splits, inputs=[train_pct, val_pct, test_pct], 
                     outputs=[split3_output, train3_file, val3_file, test3_file])
    summary_btn.click(fn=export_dataset_summary, outputs=[summary_output])
    
    gr.Markdown("""
    ---
    ### üìù ‡§Ü‡§â‡§ü‡§™‡•Å‡§ü ‡§´‡§º‡§æ‡§á‡§≤‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ / Output Files Explained:
    
    1. **audio_chunks/** - ‡§∏‡§≠‡•Ä ‡§ö‡§Ç‡§ï‡•ç‡§∏ / All chunks
    2. **filtered_chunks/** - ‡§ó‡•Å‡§£‡§µ‡§§‡•ç‡§§‡§æ ‡§´‡§º‡§ø‡§≤‡•ç‡§ü‡§∞ / Quality filtered
    3. **transcriptions.txt** - ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§∂‡§® / Transcriptions
    4. **metadata.csv** - TTS ‡§ü‡•ç‡§∞‡•á‡§®‡§ø‡§Ç‡§ó ‡§ï‡•á ‡§≤‡§ø‡§è / For TTS training
    5. **statistics/** - ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§Ü‡§Å‡§ï‡§°‡§º‡•á / Detailed stats
    
    ### üí° TTS ‡§Æ‡•â‡§°‡§≤ ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§µ‡§∞‡•ç‡§ï‡§´‡•ç‡§≤‡•ã / TTS Model Training Workflow:
    1. ‡§ë‡§°‡§ø‡§Ø‡•ã ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏ ‡§ï‡§∞‡•á‡§Ç / Process Audio ‚úÖ
    2. ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§∏‡§§‡•ç‡§Ø‡§æ‡§™‡§ø‡§§ ‡§ï‡§∞‡•á‡§Ç / Validate Dataset ‚úÖ
    3. ‡§°‡•Å‡§™‡•ç‡§≤‡•Ä‡§ï‡•á‡§ü ‡§π‡§ü‡§æ‡§è‡§Ç / Remove Duplicates üîç
    4. ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§∏‡§Ç‡§§‡•Å‡§≤‡§ø‡§§ ‡§ï‡§∞‡•á‡§Ç / Balance Dataset ‚öñÔ∏è
    5. ‡§ü‡•ç‡§∞‡•á‡§®/‡§µ‡•à‡§≤ ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§≠‡§æ‡§ú‡§ø‡§§ ‡§ï‡§∞‡•á‡§Ç / Split Train/Val üì§
    6. ‡§ü‡•ç‡§∞‡•á‡§®‡§ø‡§Ç‡§ó ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡•á‡§Ç! / Start Training! üöÄ
    
    ### üé¨ YouTube ‡§ö‡•à‡§®‡§≤ / YouTube Channel:
    **Mind Hack Secrets** - AI ‡§î‡§∞ TTS Tutorials ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§¨‡•ç‡§∏‡§ï‡•ç‡§∞‡§æ‡§á‡§¨ ‡§ï‡§∞‡•á‡§Ç!
    [https://www.youtube.com/@MindHackSecrets-o4y](https://www.youtube.com/@MindHackSecrets-o4y?sub_confirmation=1)
    """)

# Launch the app
if __name__ == "__main__":
    demo.launch(share=False)